{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4019d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from fairseq.data import (\n",
    "    data_utils,\n",
    "    Dictionary,\n",
    "    AppendTokenDataset,\n",
    "    PrependTokenDataset,\n",
    "    StripTokenDataset,\n",
    "    TokenBlockDataset,\n",
    ")\n",
    "from .denoised_dataset import BARTDenoisingDataset\n",
    "from fairseq.data.encoders.utils import get_whole_word_mask\n",
    "from fairseq.tasks import FairseqTask, register_task\n",
    "\n",
    "\n",
    "@register_task('mask_s2s')\n",
    "class DenoisingTaskS2S(FairseqTask):\n",
    "    \"\"\"\n",
    "    Denoising task for applying sequence to sequence denoising.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def add_args(parser):\n",
    "        \"\"\"Add task-specific arguments to the parser.\"\"\"\n",
    "        parser.add_argument('data', help='path to data directory')\n",
    "        parser.add_argument('--tokens-per-sample', default=512, type=int,\n",
    "                            help='max number of total tokens over all segments'\n",
    "                                 ' per sample for dataset')\n",
    "        parser.add_argument('--raw-text', default=False, action='store_true',\n",
    "                            help='load raw text dataset')\n",
    "        parser.add_argument(\n",
    "            '--sample-break-mode', default=\"eos\", type=str,\n",
    "            help='mode for breaking sentence',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--mask', default=0.3, type=float,\n",
    "            help='fraction of words/subwords that will be masked',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--mask-random', default=0.0, type=float,\n",
    "            help='instead of using [MASK], use random token this often'\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--insert', default=0.0, type=float,\n",
    "            help='insert this percentage of additional random tokens',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--mask-length', default=\"word\", type=str,\n",
    "            choices=['subword', 'word', 'span', 'span-poisson'],\n",
    "            help='mask length to choose'\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--replace-length', default=-1, type=int,\n",
    "            help='when masking N tokens, replace with 0, 1, or N tokens (use -1 for N)'\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--tokens-to-keep', default=2, type=int,\n",
    "            help=\"Don't mask first <tokens-to-keep> tokens\"\n",
    "        )\n",
    "\n",
    "        # following 2 arguments are required for the GPT2 BPE encoding\n",
    "        parser.add_argument(\n",
    "            '--gpt2_encoder_json', default=None, type=str,\n",
    "            help='GPT2 encoder path'\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--gpt2_vocab_bpe', default=None, type=str,\n",
    "            help='GPT2 vocab path'\n",
    "        )\n",
    "\n",
    "    def __init__(self, args, dictionary):\n",
    "        super().__init__(args)\n",
    "        self.dictionary = dictionary\n",
    "        self.seed = args.seed\n",
    "\n",
    "        # add mask token\n",
    "        self.mask_idx = self.dictionary.add_symbol('<mask>')\n",
    "\n",
    "    @classmethod\n",
    "    def setup_task(cls, args, **kwargs):\n",
    "        \"\"\"Setup the task.\n",
    "        \"\"\"\n",
    "        dictionary = Dictionary.load(os.path.join(args.data, 'dict.txt'))\n",
    "        print('| dictionary: {} types'.format(len(dictionary)))\n",
    "        if not hasattr(args, 'shuffle_instance'):\n",
    "            args.shuffle_instance = False\n",
    "        return cls(args, dictionary)\n",
    "\n",
    "    def load_dataset(self, split, epoch=0, combine=False, data_selector=None):\n",
    "        \"\"\"Load a given dataset split.\n",
    "\n",
    "        Args:\n",
    "            split (str): name of the split (e.g., train, valid, test)\n",
    "        \"\"\"\n",
    "\n",
    "        paths = self.args.data.split(':')\n",
    "        assert len(paths) > 0\n",
    "        data_path = paths[epoch % len(paths)]\n",
    "        split_path = os.path.join(data_path, split)\n",
    "\n",
    "        dataset = data_utils.load_indexed_dataset(\n",
    "            split_path,\n",
    "            self.dictionary,\n",
    "            self.args.dataset_impl,\n",
    "            combine=combine,\n",
    "        )\n",
    "        if dataset is None:\n",
    "            raise FileNotFoundError('Dataset not found: {} ({})'.format(split, split_path))\n",
    "\n",
    "        dataset = StripTokenDataset(dataset, self.dictionary.eos())\n",
    "\n",
    "        # create continuous blocks of tokens\n",
    "        dataset = TokenBlockDataset(\n",
    "                dataset,\n",
    "                dataset.sizes,\n",
    "                self.args.tokens_per_sample - 2,  # one less for <s> and one for </s>\n",
    "                pad=self.dictionary.pad(),\n",
    "                eos=self.dictionary.eos(),\n",
    "                break_mode=\"eos\",\n",
    "                document_sep_len=0\n",
    "        )\n",
    "\n",
    "        # prepend beginning-of-sentence token (<s>, equiv. to [CLS] in BERT)\n",
    "        dataset = PrependTokenDataset(dataset, self.source_dictionary.bos())\n",
    "        dataset = AppendTokenDataset(dataset, self.source_dictionary.eos())\n",
    "\n",
    "        mask_whole_words = get_whole_word_mask(self.args, self.source_dictionary) \\\n",
    "            if self.args.mask_length != 'subword' else None\n",
    "\n",
    "        self.datasets[split] = BARTDenoisingDataset(\n",
    "            dataset, dataset.sizes, self.dictionary, self.mask_idx,\n",
    "            mask_whole_words, shuffle=self.args.shuffle_instance,\n",
    "            seed=self.seed, args=self.args\n",
    "        )\n",
    "        print(\n",
    "            \"| Split: {0}, Loaded {1} samples of denoising_dataset\".format(\n",
    "                split,\n",
    "                len(self.datasets[split]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Return the max sentence length allowed by the task.\"\"\"\n",
    "        #return (self.args.max_source_positions, self.args.max_target_positions)\n",
    "        return (1024, 1024)\n",
    "\n",
    "    @property\n",
    "    def source_dictionary(self):\n",
    "        \"\"\"Return the source :class:`~fairseq.data.Dictionary`.\"\"\"\n",
    "        return self.dictionary\n",
    "\n",
    "    @property\n",
    "    def target_dictionary(self):\n",
    "        \"\"\"Return the target :class:`~fairseq.data.Dictionary`.\"\"\"\n",
    "        return self.dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c89ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from fairseq.data import data_utils, FairseqDataset\n",
    "\n",
    "\n",
    "def collate(\n",
    "    samples,\n",
    "    pad_idx,\n",
    "    eos_idx,\n",
    "    vocab,\n",
    "    left_pad_source=False,\n",
    "    left_pad_target=False,\n",
    "    input_feeding=True,\n",
    "):\n",
    "    assert input_feeding\n",
    "    if len(samples) == 0:\n",
    "        return {}\n",
    "\n",
    "    def merge(key, left_pad, move_eos_to_beginning=False):\n",
    "        return data_utils.collate_tokens(\n",
    "            [s[key] for s in samples],\n",
    "            pad_idx, eos_idx, left_pad, move_eos_to_beginning,\n",
    "        )\n",
    "\n",
    "    id = torch.LongTensor([s['id'] for s in samples])\n",
    "    src_tokens = merge('source', left_pad=left_pad_source)\n",
    "    # sort by descending source length\n",
    "    src_lengths = torch.LongTensor([s['source'].numel() for s in samples])\n",
    "    src_lengths, sort_order = src_lengths.sort(descending=True)\n",
    "    id = id.index_select(0, sort_order)\n",
    "    src_tokens = src_tokens.index_select(0, sort_order)\n",
    "\n",
    "    prev_output_tokens = None\n",
    "    target = None\n",
    "    if samples[0].get('target', None) is not None:\n",
    "        target = merge('target', left_pad=left_pad_target)\n",
    "        target = target.index_select(0, sort_order)\n",
    "        ntokens = sum(len(s['target']) for s in samples)\n",
    "\n",
    "        if input_feeding:\n",
    "            # we create a shifted version of targets for feeding the\n",
    "            # previous output token(s) into the next decoder step\n",
    "            prev_output_tokens = merge(\n",
    "                'target',\n",
    "                left_pad=left_pad_target,\n",
    "                move_eos_to_beginning=True,\n",
    "            )\n",
    "            prev_output_tokens = prev_output_tokens.index_select(0, sort_order)\n",
    "    else:\n",
    "        ntokens = sum(len(s['source']) for s in samples)\n",
    "\n",
    "    batch = {\n",
    "        'id': id,\n",
    "        'ntokens': ntokens,\n",
    "        'net_input': {\n",
    "            'src_tokens': src_tokens,\n",
    "            'src_lengths': src_lengths,\n",
    "        },\n",
    "        'target': target,\n",
    "        'nsentences': samples[0]['source'].size(0),\n",
    "    }\n",
    "    if prev_output_tokens is not None:\n",
    "        batch['net_input']['prev_output_tokens'] = prev_output_tokens\n",
    "    return batch\n",
    "\n",
    "\n",
    "class BARTDenoisingDataset(FairseqDataset):\n",
    "    \"\"\"\n",
    "    A wrapper around TokenBlockDataset for BART dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (TokenBlockDataset): dataset to wrap\n",
    "        sizes (List[int]): sentence lengths\n",
    "        vocab (~fairseq.data.Dictionary): vocabulary\n",
    "        mask_idx (int): dictionary index used for masked token\n",
    "        mask_whole_words: only mask whole words. This should be a byte mask\n",
    "            over vocab indices, indicating whether it is the beginning of a\n",
    "            word. We will extend any mask to encompass the whole word.\n",
    "        shuffle (bool, optional): shuffle the elements before batching.\n",
    "          Default: ``True``\n",
    "        seed: Seed for random number generator for reproducibility.\n",
    "        args: argparse arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        sizes,\n",
    "        vocab,\n",
    "        mask_idx,\n",
    "        mask_whole_words,\n",
    "        shuffle,\n",
    "        seed,\n",
    "        args\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.sizes = sizes\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.mask_idx = mask_idx\n",
    "        self.mask_whole_word = mask_whole_words\n",
    "        self.mask_ratio = args.mask\n",
    "        self.random_ratio = args.mask_random\n",
    "        self.insert_ratio = args.insert\n",
    "        self.tokens_to_keep = args.tokens_to_keep\n",
    "\n",
    "        if args.bpe != 'gpt2':\n",
    "            self.full_stop_index = self.vocab.index(\".\")\n",
    "        else:\n",
    "            assert args.bpe == 'gpt2'\n",
    "            self.full_stop_index = self.vocab.index('13')\n",
    "\n",
    "        self.tab_sep_index = self.vocab.index('\\t')\n",
    "        self.replace_length = args.replace_length\n",
    "        if not self.replace_length in [-1, 0, 1]:\n",
    "            raise (f'invalid arg: replace_length={self.replace_length}')\n",
    "        if not args.mask_length in ['subword', 'word', 'span', 'span-poisson']:\n",
    "            raise (f'invalid arg: mask-length={args.mask_length}')\n",
    "        if args.mask_length == 'subword' and not args.replace_length in [0, 1]:\n",
    "            raise (f'if using subwords, use replace-length=1 or 0')\n",
    "\n",
    "        self.is_span_mask = (args.mask_length == 'span')\n",
    "        self.mask_span_distribution = None\n",
    "        if args.mask_length == 'span-poisson':\n",
    "            _lambda = args.poisson_lambda\n",
    "\n",
    "            lambda_to_the_k = 1\n",
    "            e_to_the_minus_lambda = math.exp(-_lambda)\n",
    "            k_factorial = 1\n",
    "            ps = []\n",
    "            for k in range(0, 128):\n",
    "                ps.append(e_to_the_minus_lambda * lambda_to_the_k / k_factorial)\n",
    "                lambda_to_the_k *= _lambda\n",
    "                k_factorial *= (k + 1)\n",
    "                if ps[-1] < 0.0000001:\n",
    "                    break\n",
    "            ps = torch.FloatTensor(ps)\n",
    "            self.mask_span_distribution = torch.distributions.Categorical(ps)\n",
    "\n",
    "        self.epoch = 0\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "    def set_epoch(self, epoch, **unused):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with data_utils.numpy_seed(self.seed, self.epoch, index):\n",
    "            tokens = self.dataset[index]\n",
    "            assert tokens[-1] == self.vocab.eos()\n",
    "            source, target = tokens, tokens.clone()\n",
    "\n",
    "            if self.mask_ratio > 0:\n",
    "                if self.is_span_mask:\n",
    "                    source = self.add_multiple_words_mask(source, self.mask_ratio)\n",
    "                else:\n",
    "                    source = self.add_whole_word_mask(source, self.mask_ratio)\n",
    "\n",
    "        assert (source >= 0).all()\n",
    "        assert (source[1:-1] >= 1).all()\n",
    "        assert (source <= len(self.vocab)).all()\n",
    "        assert source[0] == self.vocab.bos()\n",
    "        assert source[-1] == self.vocab.eos()\n",
    "        return {\n",
    "            'id': index,\n",
    "            'source': source,\n",
    "            'target': target,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def word_starts(self, source):\n",
    "        if self.mask_whole_word is not None:\n",
    "            is_word_start = self.mask_whole_word.gather(0, source)\n",
    "        else:\n",
    "            is_word_start = torch.ones(source.size())\n",
    "        is_word_start[0] = 0\n",
    "        is_word_start[-1] = 0\n",
    "\n",
    "        is_word_start[1] = 0  # exclude the first word. Label word\n",
    "        # for i in range(1, self.tokens_to_keep+1):\n",
    "        #     is_word_start[i] = 0\n",
    "\n",
    "        return is_word_start\n",
    "\n",
    "    def add_whole_word_mask(self, source, p):\n",
    "        is_word_start = self.word_starts(source)\n",
    "        num_to_mask = int(math.ceil(is_word_start.float().sum() * p))\n",
    "        num_inserts = 0\n",
    "        if num_to_mask == 0:\n",
    "            return source\n",
    "\n",
    "        if self.mask_span_distribution is not None:\n",
    "            lengths = self.mask_span_distribution.sample(sample_shape=(num_to_mask,))\n",
    "\n",
    "            # Make sure we have enough to mask\n",
    "            cum_length = torch.cumsum(lengths, 0)\n",
    "            while cum_length[-1] < num_to_mask:\n",
    "                lengths = torch.cat([lengths, self.mask_span_distribution.sample(sample_shape=(num_to_mask,))], dim=0)\n",
    "                cum_length = torch.cumsum(lengths, 0)\n",
    "\n",
    "            # Trim to masking budget\n",
    "            i = 0\n",
    "            while cum_length[i] < num_to_mask:\n",
    "                i += 1\n",
    "            lengths[i] = num_to_mask - (0 if i == 0 else cum_length[i - 1])\n",
    "            num_to_mask = i + 1\n",
    "            lengths = lengths[:num_to_mask]\n",
    "\n",
    "            # Handle 0-length mask (inserts) separately\n",
    "            lengths = lengths[lengths > 0]\n",
    "            num_inserts = num_to_mask - lengths.size(0)\n",
    "            num_to_mask -= num_inserts\n",
    "            if num_to_mask == 0:\n",
    "                return self.add_insertion_noise(source, num_inserts / source.size(0))\n",
    "\n",
    "            assert (lengths > 0).all()\n",
    "        else:\n",
    "            lengths = torch.ones((num_to_mask,)).long()\n",
    "\n",
    "        assert is_word_start[-1] == 0\n",
    "        word_starts = is_word_start.nonzero()\n",
    "        indices = word_starts[torch.randperm(word_starts.size(0))[:num_to_mask]].squeeze(1)\n",
    "        mask_random = torch.FloatTensor(num_to_mask).uniform_() < self.random_ratio\n",
    "\n",
    "        source_length = source.size(0)\n",
    "        assert source_length - 1 not in indices\n",
    "        to_keep = torch.ones(source_length, dtype=torch.bool)\n",
    "        is_word_start[-1] = 255 # acts as a long length, so spans don't go over the end of doc\n",
    "        if self.replace_length == 0:\n",
    "            to_keep[indices] = 0\n",
    "        else:\n",
    "            # keep index, but replace it with [MASK]\n",
    "            source[indices] = self.mask_idx\n",
    "            source[indices[mask_random]] = torch.randint(1, len(self.vocab), size=(mask_random.sum(),))\n",
    "\n",
    "        if self.mask_span_distribution is not None:\n",
    "            assert len(lengths.size()) == 1\n",
    "            assert lengths.size() == indices.size()\n",
    "            lengths -= 1\n",
    "            while indices.size(0) > 0:\n",
    "                assert lengths.size() == indices.size()\n",
    "                lengths -= is_word_start[indices + 1].long()\n",
    "                uncompleted = lengths >= 0\n",
    "                indices = indices[uncompleted] + 1\n",
    "                mask_random = mask_random[uncompleted]\n",
    "                lengths = lengths[uncompleted]\n",
    "                if self.replace_length != -1:\n",
    "                    # delete token\n",
    "                    to_keep[indices] = 0\n",
    "                else:\n",
    "                    # keep index, but replace it with [MASK]\n",
    "                    source[indices] = self.mask_idx\n",
    "                    source[indices[mask_random]] = torch.randint(1, len(self.vocab), size=(mask_random.sum(),))\n",
    "        else:\n",
    "            # A bit faster when all lengths are 1\n",
    "            while indices.size(0) > 0:\n",
    "                uncompleted = is_word_start[indices + 1] == 0\n",
    "                indices = indices[uncompleted] + 1\n",
    "                mask_random = mask_random[uncompleted]\n",
    "                if self.replace_length != -1:\n",
    "                    # delete token\n",
    "                    to_keep[indices] = 0\n",
    "                else:\n",
    "                    # keep index, but replace it with [MASK]\n",
    "                    source[indices] = self.mask_idx\n",
    "                    source[indices[mask_random]] = torch.randint(1, len(self.vocab), size=(mask_random.sum(),))\n",
    "\n",
    "                assert source_length - 1 not in indices\n",
    "\n",
    "        source = source[to_keep]\n",
    "\n",
    "        if num_inserts > 0:\n",
    "            source = self.add_insertion_noise(source, num_inserts / source.size(0))\n",
    "\n",
    "        return source\n",
    "\n",
    "    def add_multiple_words_mask(self, source, p):\n",
    "        is_word_start = self.word_starts(source)\n",
    "        num_to_mask = int(math.ceil(is_word_start.float().sum() * p))\n",
    "        if num_to_mask == 0:\n",
    "            return source\n",
    "\n",
    "        assert is_word_start[-1] == 0\n",
    "        word_starts = is_word_start.nonzero()\n",
    "        start_index = word_starts.size(0)-num_to_mask\n",
    "        if start_index < 1:\n",
    "            print(source, is_word_start)\n",
    "            return source\n",
    "\n",
    "        mask_word_start_id = np.random.randint(start_index)\n",
    "\n",
    "        source_length = source.size(0)\n",
    "        to_keep = torch.ones(source_length, dtype=torch.bool)\n",
    "        is_word_start[-1] = 255 # acts as a long length, so spans don't go over the end of doc\n",
    "\n",
    "        # keep first index, but replace it with [MASK], and delete remaining index\n",
    "        source[word_starts[mask_word_start_id]] = self.mask_idx\n",
    "        #assert mask_word_start_id+num_to_mask < word_starts.size(0)\n",
    "        #assert (word_starts[mask_word_start_id].item()+num_to_mask) < source_length\n",
    "        try:\n",
    "            for ind in range(word_starts[mask_word_start_id]+1, word_starts[mask_word_start_id+num_to_mask]):\n",
    "                to_keep[ind] = 0\n",
    "        except IndexError:\n",
    "            print(\"Index error\", source, is_word_start)\n",
    "            pass\n",
    "\n",
    "        source = source[to_keep]\n",
    "        return source\n",
    "\n",
    "    def collater(self, samples):\n",
    "        \"\"\"Merge a list of samples to form a mini-batch.\n",
    "        Args:\n",
    "            samples (List[dict]): samples to collate\n",
    "        Returns:\n",
    "            dict: a mini-batch of data\n",
    "        \"\"\"\n",
    "        return collate(samples, self.vocab.pad(), self.vocab.eos(), self.vocab)\n",
    "\n",
    "    def num_tokens(self, index):\n",
    "        \"\"\"Return the number of tokens in a sample. This value is used to\n",
    "        enforce ``--max-tokens`` during batching.\"\"\"\n",
    "        return self.sizes[index]\n",
    "\n",
    "    def size(self, index):\n",
    "        \"\"\"Return an example's size as a float or tuple. This value is used when\n",
    "        filtering a dataset with ``--max-positions``.\"\"\"\n",
    "        return self.sizes[index]\n",
    "\n",
    "    def ordered_indices(self):\n",
    "        \"\"\"Return an ordered list of indices. Batches will be constructed based\n",
    "        on this order.\"\"\"\n",
    "        if self.shuffle:\n",
    "            indices = np.random.permutation(len(self))\n",
    "        else:\n",
    "            indices = np.arange(len(self))\n",
    "        return indices[np.argsort(self.sizes[indices], kind='mergesort')]\n",
    "\n",
    "    def prefetch(self, indices):\n",
    "        self.src.prefetch(indices)\n",
    "        self.tgt.prefetch(indices)\n",
    "\n",
    "    @property\n",
    "    def supports_prefetch(self):\n",
    "        return (\n",
    "            hasattr(self.src, 'supports_prefetch')\n",
    "            and self.src.supports_prefetch\n",
    "            and hasattr(self.tgt, 'supports_prefetch')\n",
    "            and self.tgt.supports_prefetch\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from fairseq.data import (\n",
    "    data_utils,\n",
    "    Dictionary,\n",
    "    AppendTokenDataset,\n",
    "    PrependTokenDataset,\n",
    "    StripTokenDataset,\n",
    "    TokenBlockDataset,\n",
    ")\n",
    "from .denoised_dataset import BARTDenoisingDataset\n",
    "from fairseq.data.encoders.utils import get_whole_word_mask\n",
    "from fairseq.tasks import FairseqTask, register_task\n",
    "\n",
    "\n",
    "@register_task('mask_s2s')\n",
    "class DenoisingTaskS2S(FairseqTask):\n",
    "    \"\"\"\n",
    "    Denoising task for applying sequence to sequence denoising.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def add_args(parser):\n",
    "        \"\"\"Add task-specific arguments to the parser.\"\"\"\n",
    "        parser.add_argument('data', help='path to data directory')\n",
    "        parser.add_argument('--tokens-per-sample', default=512, type=int,\n",
    "                            help='max number of total tokens over all segments'\n",
    "                                 ' per sample for dataset')\n",
    "        parser.add_argument('--raw-text', default=False, action='store_true',\n",
    "                            help='load raw text dataset')\n",
    "        parser.add_argument(\n",
    "            '--sample-break-mode', default=\"eos\", type=str,\n",
    "            help='mode for breaking sentence',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--mask', default=0.3, type=float,\n",
    "            help='fraction of words/subwords that will be masked',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--mask-random', default=0.0, type=float,\n",
    "            help='instead of using [MASK], use random token this often'\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--insert', default=0.0, type=float,\n",
    "            help='insert this percentage of additional random tokens',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--mask-length', default=\"word\", type=str,\n",
    "            choices=['subword', 'word', 'span', 'span-poisson'],\n",
    "            help='mask length to choose'\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--replace-length', default=-1, type=int,\n",
    "            help='when masking N tokens, replace with 0, 1, or N tokens (use -1 for N)'\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--tokens-to-keep', default=2, type=int,\n",
    "            help=\"Don't mask first <tokens-to-keep> tokens\"\n",
    "        )\n",
    "\n",
    "        # following 2 arguments are required for the GPT2 BPE encoding\n",
    "        parser.add_argument(\n",
    "            '--gpt2_encoder_json', default=None, type=str,\n",
    "            help='GPT2 encoder path'\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--gpt2_vocab_bpe', default=None, type=str,\n",
    "            help='GPT2 vocab path'\n",
    "        )\n",
    "\n",
    "    def __init__(self, args, dictionary):\n",
    "        super().__init__(args)\n",
    "        self.dictionary = dictionary\n",
    "        self.seed = args.seed\n",
    "\n",
    "        # add mask token\n",
    "        self.mask_idx = self.dictionary.add_symbol('<mask>')\n",
    "\n",
    "    @classmethod\n",
    "    def setup_task(cls, args, **kwargs):\n",
    "        \"\"\"Setup the task.\n",
    "        \"\"\"\n",
    "        dictionary = Dictionary.load(os.path.join(args.data, 'dict.txt'))\n",
    "        print('| dictionary: {} types'.format(len(dictionary)))\n",
    "        if not hasattr(args, 'shuffle_instance'):\n",
    "            args.shuffle_instance = False\n",
    "        return cls(args, dictionary)\n",
    "\n",
    "    def load_dataset(self, split, epoch=0, combine=False, data_selector=None):\n",
    "        \"\"\"Load a given dataset split.\n",
    "\n",
    "        Args:\n",
    "            split (str): name of the split (e.g., train, valid, test)\n",
    "        \"\"\"\n",
    "\n",
    "        paths = self.args.data.split(':')\n",
    "        assert len(paths) > 0\n",
    "        data_path = paths[epoch % len(paths)]\n",
    "        split_path = os.path.join(data_path, split)\n",
    "\n",
    "        dataset = data_utils.load_indexed_dataset(\n",
    "            split_path,\n",
    "            self.dictionary,\n",
    "            self.args.dataset_impl,\n",
    "            combine=combine,\n",
    "        )\n",
    "        if dataset is None:\n",
    "            raise FileNotFoundError('Dataset not found: {} ({})'.format(split, split_path))\n",
    "\n",
    "        dataset = StripTokenDataset(dataset, self.dictionary.eos())\n",
    "\n",
    "        # create continuous blocks of tokens\n",
    "        dataset = TokenBlockDataset(\n",
    "                dataset,\n",
    "                dataset.sizes,\n",
    "                self.args.tokens_per_sample - 2,  # one less for <s> and one for </s>\n",
    "                pad=self.dictionary.pad(),\n",
    "                eos=self.dictionary.eos(),\n",
    "                break_mode=\"eos\",\n",
    "                document_sep_len=0\n",
    "        )\n",
    "\n",
    "        # prepend beginning-of-sentence token (<s>, equiv. to [CLS] in BERT)\n",
    "        dataset = PrependTokenDataset(dataset, self.source_dictionary.bos())\n",
    "        dataset = AppendTokenDataset(dataset, self.source_dictionary.eos())\n",
    "\n",
    "        mask_whole_words = get_whole_word_mask(self.args, self.source_dictionary) \\\n",
    "            if self.args.mask_length != 'subword' else None\n",
    "\n",
    "        self.datasets[split] = BARTDenoisingDataset(\n",
    "            dataset, dataset.sizes, self.dictionary, self.mask_idx,\n",
    "            mask_whole_words, shuffle=self.args.shuffle_instance,\n",
    "            seed=self.seed, args=self.args\n",
    "        )\n",
    "        print(\n",
    "            \"| Split: {0}, Loaded {1} samples of denoising_dataset\".format(\n",
    "                split,\n",
    "                len(self.datasets[split]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Return the max sentence length allowed by the task.\"\"\"\n",
    "        #return (self.args.max_source_positions, self.args.max_target_positions)\n",
    "        return (1024, 1024)\n",
    "\n",
    "    @property\n",
    "    def source_dictionary(self):\n",
    "        \"\"\"Return the source :class:`~fairseq.data.Dictionary`.\"\"\"\n",
    "        return self.dictionary\n",
    "\n",
    "    @property\n",
    "    def target_dictionary(self):\n",
    "        \"\"\"Return the target :class:`~fairseq.data.Dictionary`.\"\"\"\n",
    "        return self.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b83bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env bash\n",
    "\n",
    "WARMUP_UPDATES=60\n",
    "LR=1e-05                # Peak LR for polynomial LR scheduler.\n",
    "SRC=~/PretrainedDataAugment/src\n",
    "BART_PATH=~/bart.large\n",
    "CACHE=~/CACHE\n",
    "PREFIXSIZE=3\n",
    "MAXEPOCH=30\n",
    "TASK=snips\n",
    "\n",
    "for NUMEXAMPLES in 10;\n",
    "do\n",
    "    for i in {0..14};\n",
    "    do\n",
    "    RAWDATADIR=~/datasets/${TASK}/exp_${i}_${NUMEXAMPLES}\n",
    "    DATABIN=$RAWDATADIR/jointdatabin\n",
    "\n",
    "    splits=( train dev )\n",
    "    for split in \"${splits[@]}\";\n",
    "        do\n",
    "        python $SRC/utils/bpe_encoder.py \\\n",
    "            --encoder-json $SRC/utils/gpt2_bpe/encoder.json \\\n",
    "            --vocab-bpe $SRC/utils/gpt2_bpe/vocab.bpe \\\n",
    "            --inputs $RAWDATADIR/${split}.tsv  \\\n",
    "            --outputs $RAWDATADIR/${split}_bpe.src \\\n",
    "            --workers 1 --keep-empty --tsv --dataset $TASK\n",
    "        done\n",
    "\n",
    "        fairseq-preprocess --user-dir=$SRC/bart_aug --only-source \\\n",
    "                    --task mask_s2s \\\n",
    "                    --trainpref $RAWDATADIR/train_bpe.src \\\n",
    "                    --validpref $RAWDATADIR/dev_bpe.src \\\n",
    "                    --destdir $DATABIN \\\n",
    "                    --srcdict $BART_PATH/dict.txt\n",
    "\n",
    "        # Run data generation with different noise setting\n",
    "        for mr in 40;\n",
    "          do\n",
    "            MRATIO=0.${mr}\n",
    "            for MASKLEN in word span;\n",
    "                do\n",
    "                MODELDIR=$RAWDATADIR/bart_${MASKLEN}_mask_${MRATIO}_checkpoints\n",
    "                mkdir $MODELDIR\n",
    "\n",
    "                CUDA_VISIBLE_DEVICES=0 fairseq-train  $DATABIN/ \\\n",
    "                    --user-dir=$SRC/bart_aug \\\n",
    "                    --restore-file $BART_PATH/model.pt \\\n",
    "                    --arch bart_large \\\n",
    "                    --task mask_s2s \\\n",
    "                    --bpe gpt2 \\\n",
    "                    --gpt2_encoder_json $SRC/utils/gpt2_bpe/encoder.json \\\n",
    "                    --gpt2_vocab_bpe $SRC/utils/gpt2_bpe/vocab.bpe \\\n",
    "                    --layernorm-embedding \\\n",
    "                    --share-all-embeddings \\\n",
    "                    --save-dir $MODELDIR\\\n",
    "                    --seed $i \\\n",
    "                    --share-decoder-input-output-embed \\\n",
    "                    --reset-optimizer --reset-dataloader --reset-meters \\\n",
    "                    --required-batch-size-multiple 1 \\\n",
    "                    --max-tokens 2000 \\\n",
    "                    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "                    --dropout 0.1 --attention-dropout 0.1 \\\n",
    "                    --weight-decay 0.01 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
    "                    --clip-norm 0.0 \\\n",
    "                    --lr-scheduler polynomial_decay --lr $LR \\\n",
    "                    --warmup-updates $WARMUP_UPDATES \\\n",
    "                    --replace-length 1 --mask-length $MASKLEN --mask $MRATIO --fp16 --update-freq 1 \\\n",
    "                    --max-epoch $MAXEPOCH --no-epoch-checkpoints > $MODELDIR/bart.log\n",
    "\n",
    "               CUDA_VISIBLE_DEVICES=0 fairseq-generate $DATABIN \\\n",
    "                        --user-dir=$SRC/bart_aug \\\n",
    "                        --task mask_s2s --tokens-to-keep $PREFIXSIZE \\\n",
    "                        --seed ${i} \\\n",
    "                        --bpe gpt2 \\\n",
    "                        --gpt2_encoder_json $SRC/utils/gpt2_bpe/encoder.json \\\n",
    "                        --gpt2_vocab_bpe $SRC/utils/gpt2_bpe/vocab.bpe \\\n",
    "                        --path $MODELDIR/checkpoint_best.pt \\\n",
    "                        --replace-length 1 --mask-length $MASKLEN --mask $MRATIO \\\n",
    "                        --batch-size 64 --beam 5 --lenpen 5 \\\n",
    "                        --no-repeat-ngram-size 2 \\\n",
    "                        --max-len-b 50 --prefix-size $PREFIXSIZE \\\n",
    "                        --gen-subset train > $MODELDIR/bart_l5_${PREFIXSIZE}.gen\n",
    "\n",
    "                grep ^H $MODELDIR/bart_l5_${PREFIXSIZE}.gen | cut -f3 > $MODELDIR/bart_l5_gen_${PREFIXSIZE}.bpe\n",
    "                rm $MODELDIR/checkpoint_last.pt\n",
    "                python $SRC/utils/bpe_encoder.py \\\n",
    "                        --encoder-json $SRC/utils/gpt2_bpe/encoder.json \\\n",
    "                        --vocab-bpe $SRC/utils/gpt2_bpe/vocab.bpe \\\n",
    "                        --inputs $MODELDIR/bart_l5_gen_${PREFIXSIZE}.bpe \\\n",
    "                        --outputs $MODELDIR/bart_l5_gen_${PREFIXSIZE}.tsv --dataset $TASK \\\n",
    "                        --workers 1 --keep-empty --decode --tsv\n",
    "            done\n",
    "        done\n",
    "\n",
    "        ########################\n",
    "        ## BART Classifier\n",
    "        ########################\n",
    "\n",
    "         for mr in 40;\n",
    "              do\n",
    "                MRATIO=0.${mr}\n",
    "                for MASKLEN in span word;\n",
    "                    do\n",
    "                     MODELDIR=$RAWDATADIR/bart_${MASKLEN}_mask_${MRATIO}_checkpoints\n",
    "\n",
    "                    cat $RAWDATADIR/train.tsv $MODELDIR/bart_l5_gen_${PREFIXSIZE}.tsv > $MODELDIR/train.tsv\n",
    "                    cp $RAWDATADIR/test.tsv $MODELDIR/test.tsv\n",
    "                    cp $RAWDATADIR/dev.tsv $MODELDIR/dev.tsv\n",
    "                    python $SRC/bert_aug/bert_classifier.py --task $TASK --data_dir $MODELDIR --seed ${i} --cache $CACHE > $RAWDATADIR/bert_bart_l5_${MASKLEN}_mask_${MRATIO}_prefix_${PREFIXSIZE}.log\n",
    "                done\n",
    "           done\n",
    "    done\n",
    "done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
